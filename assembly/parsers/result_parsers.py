# -*- coding: utf-8 -*-
"""cpo-pipeline.assembly.parsers.result_parsers

This module provides functions for parsing result files generated by tools
during the QC & Assembly phase of the cpo-pipeline.
"""

import pandas

def parse_kraken_result(path_to_kraken_result):
    """
    Args:
        path_to_kraken_result (str): Path to the kraken report file.

    Returns:
        dict: Parsed kraken report with species-level results.
        For example:
        { "Escherichia coli": { "fragment_percent": 80.2,
                                "fragment_count_root": 120321,
                                "fragment_count_taxon": 841210,
                                "rank_code": "S",
                                "ncbi_taxon_id": "562",
                                "name": "Escherichia coli",
                                "row": " 84.08\t195563\t192561\tS\t562\tEscherichia coli"
                              }
          "Another species": { "fragment_percent": 12.1,
                               ...
                             }
        }
        See kraken manual for more detail on report fields:
        http://ccb.jhu.edu/software/kraken/MANUAL.html#sample-report-output-format
    """
    kraken_result = {}
    parsed_kraken_report_full = pandas.read_csv(path_to_kraken_result, delimiter='\t', header=None)
    #find all the rows with species level information and discard the rest
    parsed_kraken_report_species_only = parsed_kraken_report_full.loc[
        (parsed_kraken_report_full[3] == "S")
    ]
    for i in range(len(parsed_kraken_report_species_only.index)):
        #find all species above 1%
        if (float((str(parsed_kraken_report_species_only.iloc[i, 0])).strip("%")) > 1.00):
            kraken_species_record = {}
            kraken_species_record['fragment_percent'] = float((str(parsed_kraken_report_species_only.iloc[i, 0])).strip("%"))
            kraken_species_record['fragment_count_root'] = int(parsed_kraken_report_species_only.iloc[i,1])
            kraken_species_record['fragment_count_taxon'] = int(parsed_kraken_report_species_only.iloc[i,2])
            kraken_species_record['rank_code'] = parsed_kraken_report_species_only.iloc[i,3]
            kraken_species_record['ncbi_taxon_id'] = parsed_kraken_report_species_only.iloc[i,4]
            kraken_species_record['name'] = parsed_kraken_report_species_only.iloc[i,5].strip()
            kraken_species_record['row'] = "\t".join(str(x) for x in parsed_kraken_report_species_only.iloc[i].tolist())
            kraken_result[kraken_species_record['name']] = kraken_species_record
    return kraken_result

def parse_fastqc_result(path_to_R1_qc, path_to_R2_qc):
    """
    Args:
        path_to_R1_qc (str): Path to the fastqc R1 report directory.
        path_to_R2_qc (str): Path to the fastqc R2 report directory.

    Returns:
        tuple(dict1, dict2) where:
        dict1: Parsed fastqc R1 report.
               All values except 'fastqc_html_blob' are either "PASS", "WARN", or "FAIL".
        For example:
        { "basic": "PASS",
          "per_base_sequence_quality": "PASS",
          "per_tile_sequence_quality": "PASS",
          "per_sequence_quality_scores": "PASS",
          "per_base_sequence_content": "WARN",
          "per_sequence_gc_content": "FAIL",
          "per_base_n_content": "PASS",
          "sequence_length_distribution": "WARN",
          "sequence_duplication_levels": "PASS",
          "overrepresented_sequences": "WARN",
          "adapter_content": "PASS",
          "fastqc_html_blob": "<html><head><title>BC18-Eco016_R1.fastq.gz FastQC Report</title><style type="text/css">..."
        }
        dict2: Parsed fastqc R2 report.
               Structure is identical to Parsed fastqc R1 report.
    """
    fastqc_R1_summary = pandas.read_csv(path_to_R1_qc + "summary.txt", delimiter='\t', header=None)
    fastqc_R1_summary_labels = fastqc_R1_summary[0].tolist()

    fastqc_R1 = {}
    fastqc_R1['basic'] = fastqc_R1_summary_labels[0]
    fastqc_R1['per_base_sequence_quality'] = fastqc_R1_summary_labels[1]
    fastqc_R1['per_tile_sequence_quality'] = fastqc_R1_summary_labels[2]
    fastqc_R1['per_sequence_quality_scores'] = fastqc_R1_summary_labels[3]
    fastqc_R1['per_base_sequence_content'] = fastqc_R1_summary_labels[4]
    fastqc_R1['per_sequence_gc_content'] = fastqc_R1_summary_labels[5]
    fastqc_R1['per_base_n_content'] = fastqc_R1_summary_labels[6]
    fastqc_R1['sequence_length_distribution'] = fastqc_R1_summary_labels[7]
    fastqc_R1['sequence_duplication_levels'] = fastqc_R1_summary_labels[8]
    fastqc_R1['overrepresented_sequences'] = fastqc_R1_summary_labels[9]
    fastqc_R1['adapter_content'] = fastqc_R1_summary_labels[10]
    with open(path_to_R1_qc + "fastqc_report.html", "r") as input:
        fastqc_R1['fastqc_html_blob'] = input.read()

    fastqc_R2_summary = pandas.read_csv(path_to_R2_qc + "summary.txt", delimiter='\t', header=None)
    fastqc_R2_summary_labels = fastqc_R2_summary[0].tolist()

    fastqc_R2 = {}
    fastqc_R2['basic'] = fastqc_R2_summary_labels[0]
    fastqc_R2['per_base_sequence_quality'] = fastqc_R2_summary_labels[1]
    fastqc_R2['per_tile_sequence_quality'] = fastqc_R2_summary_labels[2]
    fastqc_R2['per_sequence_quality_scores'] = fastqc_R2_summary_labels[3]
    fastqc_R2['per_base_sequence_content'] = fastqc_R2_summary_labels[4]
    fastqc_R2['per_sequence_gc_content'] = fastqc_R2_summary_labels[5]
    fastqc_R2['per_base_n_content'] = fastqc_R2_summary_labels[6]
    fastqc_R2['sequence_length_distribution'] = fastqc_R2_summary_labels[7]
    fastqc_R2['sequence_duplication_levels'] = fastqc_R2_summary_labels[8]
    fastqc_R2['overrepresented_sequences'] = fastqc_R2_summary_labels[9]
    fastqc_R2['adapter_content'] = fastqc_R2_summary_labels[10]
    with open(path_to_R2_qc + "fastqc_report.html", "r") as input:
        fastqc_R2['fastqc_html_blob'] = input.read()

    return fastqc_R1, fastqc_R2

def parse_mash_genome_result(path_to_mash_screen, size, depth):
    """
    Args:
        path_to_mash_screen (str): Path to the mash screen report file.
        size (int):
        depth (int):

    Returns:
        tuple(dict, boolean) where:
        dict: Parsed mash screen report
        For example:
        { "Escherichia coli": { "size": ,
                                "depth": ,
                                "identity": 0.998694,
                                "shared_hashes": "972/1000",
                                "median_multiplicity": 16,
                                "p_value": 0.00,
                                "query_ID": "GCF_001519515.1_ASM151951v1_genomic.fna.gz",
                                "query_comment": "[219 seqs] NZ_LQSN01000001.1 Escherichia coli strain GN02215 GCID_ECOLID_00024_NODE_1.ctg_1, whole genome shotgun sequence [...]",
                                "accession": "[219 seqs] NZ_LQSN01000001.1 Escherichia coli strain GN02215 GCID_ECOLID_00024_NODE_1.ctg_1, whole genome shotgun sequence [...]",
                                "row": "0.998649\t972/1000\t16\t0\tGCF_001519515.1_ASM151951v1_genomic.fna.gz\t[219 seqs] NZ_LQSN01000001.1 Escherichia coli strain GN02215 GCID_ECOLID_00024_NODE_1.ctg_1, whole genome shotgun sequence [...]",
                                "gcf": ["GCF", "001", "519", "515"],
                                "assembly": "GCF_001519515.1_ASM151951v1",
                                "species": "Escherichia coli"
                              }
          "Another species": { "size": ,
                               ...
                             }
        }
        See mash docs for more info on mash screen report file:
        https://mash.readthedocs.io/en/latest/tutorials.html#screening-a-read-set-for-containment-of-refseq-genomes
        boolean: true if phiX is present in top hits, false if absent
    
    """
    mash_hits = {}
    phiX = False
    
    mash_screen_report = pandas.read_csv(path_to_mash_screen, delimiter='\t', header=None)
    # Example mash screen report record (actual report has no header and is tab-delimited):
    # identity    shared-hashes    median-multiplicity    p-value    query-ID                                    query-comment]
    # 0.998697    973/1000         71                     0          GCF_000958965.1_matepair4_genomic.fna.gz    [59 seqs] NZ_LAFU01000001.1 Klebsiella pneumoniae strain CDPH5262 contig000001, whole genome shotgun sequence [...]
    # parse mash result, using winner takes all
    scores = mash_screen_report[1].values
    score_cutoff = int(scores[0][:scores[0].index("/")]) - 300 #find the cut off value to use for filtering the report (300 below max)
    index = 0
    #find hits with score within top 300
    for score in scores:
        parsed_score = int(score[:score.index("/")])
        if parsed_score >= score_cutoff:
            index+=1
        else:
            break

    #parse what the species are.
    for i in range(index):
        mash_record = {}
        mash_record['size'] = size
        mash_record['depth'] = depth
        mash_record['identity'] = float(mash_screen_report.ix[i, 0])
        mash_record['shared_hashes'] = mash_screen_report.ix[i, 1]
        mash_record['median_multiplicity'] = int(mash_screen_report.ix[i, 2])
        mash_record['p_value'] = float(mash_screen_report.ix[i, 3])
        mash_record['query_ID'] = mash_screen_report.ix[i, 4]
        mash_record['query_comment'] = mash_screen_report.ix[i, 5]
        mash_record['accession'] = mash_record['query_comment']
        mash_record['row'] = "\t".join(str(x) for x in mash_screen_report.ix[i].tolist())
        qID = mash_record['query_ID']
        # find gcf accession
        gcf = (qID[:qID.find("_",5)]).replace("_","")
        gcf = [gcf[i:i+3] for i in range(0, len(gcf), 3)]
        mash_record['gcf'] = gcf 
        # find assembly name
        mash_record['assembly'] = qID[:qID.find("_genomic.fna.gz")]

        if (mash_record['query_comment'].find("phiX") > -1): #theres phix in top hits, just ignore
            phiX = True
            mash_record['species'] = "PhiX"
        else: #add the non-phix hits to a list
            species_name_start = int(mash_record['query_comment'].index(".")) + 3
            species_name_stop = int (mash_record['query_comment'].index(","))
            mash_record['species'] = str(mash_record['query_comment'])[species_name_start: species_name_stop]
            mash_hits[mash_record['species']] = mash_record
    return mash_hits, phiX

def parse_mash_plasmid_result(path_to_mash_screen, size, depth):
    """
    Args:
        path_to_mash_screen (str): Path to the mash screen report file.
        size (int):
        depth (int):

    Returns:
        dict: Parsed mash screen report
        For example:
        { "NZ_CP010882.1": { "size": ,
                             "depth": ,
                             "identity": 0.992291,
                             "shared_hashes": "850/1000",
                             "median_multiplicity": 19,
                             "p_value": 0.00,
                             "query_ID": "ref|NZ_CP010882.1|",
                             "query_comment": "Escherichia coli strain MNCRE44 plasmid pMNCRE44_6, complete sequence",
                             "accession": "NZ_CP010882.1",
                             "row": "0.992291\t850/1000\t19\t0\tref|NZ_CP010882.1|\tEscherichia coli strain MNCRE44 plasmid pMNCRE44_6, complete sequence",
                             "species": ""
                           }
          "Another accession": { "size": ,
                                 ...
                               }
        }

        See mash docs for more info on mash screen report file:
        https://mash.readthedocs.io/en/latest/tutorials.html#screening-a-read-set-for-containment-of-refseq-genomes
    """
    mash_screen_report = pandas.read_csv(path_to_mash_screen, delimiter='\t', header=None)

    #parse mash result, using winner takes all
    scores = mash_screen_report[1].values
    score_cutoff = int(scores[0][:scores[0].index("/")]) - 100
    index = 0

    #find hits with score > (max_score - 100)
    for score in scores:
        parsed_score = int(score[:score.index("/")])
        if parsed_score >= score_cutoff:
            index+=1
        else:
            break

    mash_plasmid_hits = {}
    #parse what the species are.
    for i in range(index):
        mash_record = {}
        mash_record['size'] = size
        mash_record['depth'] = depth
        mash_record['identity'] = float(mash_screen_report.ix[i, 0])
        mash_record['shared_hashes'] = mash_screen_report.ix[i, 1]
        mash_record['median_multiplicity'] = int(mash_screen_report.ix[i, 2])
        mash_record['p_value'] = float(mash_screen_report.ix[i, 3])
        mash_record['query_ID'] = mash_screen_report.ix[i, 4] #accession
        mash_record['query_comment'] = mash_screen_report.ix[i, 5]
        mash_record['accession'] = mash_record['query_ID'][4:len(mash_record['query_ID'])-1]
        mash_record['row'] = "\t".join(str(x) for x in mash_screen_report.ix[i].tolist())
        mash_record['species'] = ""
        if (mash_record['identity'] >= 0.97):
            mash_plasmid_hits[mash_record['accession']] = mash_record
    return mash_plasmid_hits

def parse_read_stats(path_to_mash_log, path_to_total_bp):
    """
    Args:
        path_to_mash_log (str): Path to the mash log file.
        path_to_total_bp (str): Path to

    Returns:
        tuple(float1, float2) where:
          float1: Estimated genome size
          float2: Estimated depth (total bases / genome size)
    """
    total_bp = int([line.rstrip('\n') for line in open(path_to_total_bp)][0])
    mash_log = [line.rstrip('\n') for line in open(path_to_mash_log)]
    for line in mash_log:
        if (line.find("Estimated genome size:") > -1 ):
            size = float(line[line.index(": ") + 2:])
    depth = total_bp / size
    depth = float(format(depth, '.2f'))
    return size, depth

def parse_busco_result(path_to_busco_result):
    """
    Args:
        path_to_busco_log (str): Path to the mash log file.
    Returns:
        dict: Parsed busco report
        For example:
        { "complete_single": 778,
          "complete_duplicate": 1,
          "fragmented": 1,
          "missing": 1,
        }
    Raises:
        Exception: When BUSCO output file is empty
    """
    busco_output = [line.rstrip('\n') for line in open(path_to_busco_result)]
    if (len(busco_output) > 0):
        busco_result = {}
        busco_result['complete_single'] = int(busco_output[10].split("\t")[1])
        busco_result['complete_duplicate'] = int(busco_output[11].split("\t")[1])
        busco_result['fragmented'] = int(busco_output[12].split("\t")[1])
        busco_result['missing'] = int(busco_output[13].split("\t")[1])
        busco_result['total'] = int(busco_output[14].split("\t")[1])
    else:
        raise Exception("BUSCO output file has no contents.")
    return busco_result

def parse_quast_result(pathToQuastResult):
    qResult = read(pathToQuastResult)
    qr = quastResult()

    qr.contigsCount = int(qResult[int([i for i,x in enumerate(qResult) if x.find("# contigs\t") > -1][-1])].split("\t")[1])
    qr.largestContig = int(qResult[int([i for i,x in enumerate(qResult) if x.find("Largest contig\t") > -1][-1])].split("\t")[1])
    qr.N50 = int(qResult[int([i for i,x in enumerate(qResult) if x.find("N50\t") > -1][-1])].split("\t")[1])
    qr.NG50 = int(qResult[int([i for i,x in enumerate(qResult) if x.find("NG50\t") > -1][-1])].split("\t")[1])
    qr.L50 = int(qResult[int([i for i,x in enumerate(qResult) if x.find("L50\t") > -1][-1])].split("\t")[1])
    qr.LG50 = int(qResult[int([i for i,x in enumerate(qResult) if x.find("LG50\t") > -1][-1])].split("\t")[1])
    qr.N75 = int(qResult[int([i for i,x in enumerate(qResult) if x.find("N75\t") > -1][-1])].split("\t")[1])
    qr.NG75 = int(qResult[int([i for i,x in enumerate(qResult) if x.find("NG75\t") > -1][-1])].split("\t")[1])
    qr.L75 = int(qResult[int([i for i,x in enumerate(qResult) if x.find("L75\t") > -1][-1])].split("\t")[1])
    qr.LG75 = int(qResult[int([i for i,x in enumerate(qResult) if x.find("LG75\t") > -1][-1])].split("\t")[1])
    qr.totalLength = int(qResult[int([i for i,x in enumerate(qResult) if x.find("Total length\t") > -1][-1])].split("\t")[1])
    qr.referenceLength = int(qResult[int([i for i,x in enumerate(qResult) if x.find("Reference length\t") > -1][-1])].split("\t")[1])
    qr.GC = float(qResult[int([i for i,x in enumerate(qResult) if x.find("GC (%)\t") > -1][-1])].split("\t")[1])
    qr.referenceGC = float(qResult[int([i for i,x in enumerate(qResult) if x.find("Reference GC (%)\t") > -1][-1])].split("\t")[1])
    qr.genomeFraction = float(qResult[int([i for i,x in enumerate(qResult) if x.find("Genome fraction (%)\t") > -1][-1])].split("\t")[1])
    qr.duplicationRatio = float(qResult[int([i for i,x in enumerate(qResult) if x.find("Duplication ratio\t") > -1][-1])].split("\t")[1])
    qr.data = qResult

    return qr

